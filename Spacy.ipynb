{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score,precision_score,accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from notifyme import notify\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./train_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_text'] = df.comment_text.fillna(value=\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edit username hardcore metallica f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww match background colour be seemingly stic...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man be try edit war guy constantly remove ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>not real suggestion improvement wonder section...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page s</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  explanation edit username hardcore metallica f...      0   \n",
       "1  000103f0d9cfb60f  daww match background colour be seemingly stic...      0   \n",
       "2  000113f07ec002fd  hey man be try edit war guy constantly remove ...      0   \n",
       "3  0001b41b1c6bb37e  not real suggestion improvement wonder section...      0   \n",
       "4  0001d958c54c6e35                    sir hero chance remember page s      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y,y_pred):\n",
    "    print (f\"Precision : {precision_score(y,y_pred)}\")\n",
    "    print (f\"Recall : {recall_score(y,y_pred)}\")\n",
    "    print (f\"Accuracy : {accuracy_score(y,y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.comment_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comment in df.comment_text.values:\n",
    "    for word in comment.split(\" \"):\n",
    "        words[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.pop(\"comment_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df.pop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_vect = cv.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,x,Y,y = train_test_split(features_vect,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.fit(X,Y['toxic'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.8895221394125383\n",
      "Recall : 0.5379109225874867\n",
      "Accuracy : 0.9499912265309703\n"
     ]
    }
   ],
   "source": [
    "score(y['toxic'].values,log.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, input_shape=(X.shape[1],)),\n",
    "    Activation('relu'),\n",
    "    Dense(28,),\n",
    "    Activation('relu'),\n",
    "    Dense(32,),\n",
    "    Dropout(0.3),\n",
    "    Activation('relu'),\n",
    "    Dense(2),\n",
    "    Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.3324 - acc: 0.9060\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 677us/sample - loss: 0.1927 - acc: 0.9438\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1849 - acc: 0.9528\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 654us/sample - loss: 0.1124 - acc: 0.9685\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.2095 - acc: 0.9485\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 663us/sample - loss: 0.1362 - acc: 0.9608\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1861 - acc: 0.9528\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 655us/sample - loss: 0.1350 - acc: 0.9652\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1992 - acc: 0.9548\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 664us/sample - loss: 0.1377 - acc: 0.9675\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.2088 - acc: 0.9528\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 656us/sample - loss: 0.1657 - acc: 0.9645\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1569 - acc: 0.9593\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 674us/sample - loss: 0.1144 - acc: 0.9702\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1552 - acc: 0.9568\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 724us/sample - loss: 0.1087 - acc: 0.9680\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.2712 - acc: 0.9573\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 669us/sample - loss: 0.2116 - acc: 0.9663\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1440 - acc: 0.9598\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 690us/sample - loss: 0.1118 - acc: 0.9675\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1513 - acc: 0.9540\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 661us/sample - loss: 0.1133 - acc: 0.9633\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1467 - acc: 0.9562\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 669us/sample - loss: 0.1083 - acc: 0.9668\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1511 - acc: 0.9568\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 665us/sample - loss: 0.1100 - acc: 0.9682\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1788 - acc: 0.9573\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 664us/sample - loss: 0.1350 - acc: 0.9643\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1470 - acc: 0.9567\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 704us/sample - loss: 0.1122 - acc: 0.9655\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1530 - acc: 0.9583\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 659us/sample - loss: 0.1146 - acc: 0.9667\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1459 - acc: 0.9578\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 708us/sample - loss: 0.1085 - acc: 0.9673\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1523 - acc: 0.9587\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 667us/sample - loss: 0.1161 - acc: 0.9655\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1852 - acc: 0.9587\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 674us/sample - loss: 0.1436 - acc: 0.9653\n",
      "Train on 5678 samples\n",
      "5678/5678 [==============================] - 6s 1ms/sample - loss: 0.2040 - acc: 0.9584 1s \n",
      "Epoch : 1\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1283 - acc: 0.9587\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 725us/sample - loss: 0.1009 - acc: 0.9675\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 7s 1ms/sample - loss: 0.1322 - acc: 0.9662\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 682us/sample - loss: 0.1026 - acc: 0.9723\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1487 - acc: 0.9570\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 670us/sample - loss: 0.1204 - acc: 0.9648\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1421 - acc: 0.9615\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 683us/sample - loss: 0.1063 - acc: 0.9683\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1481 - acc: 0.9605\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 678us/sample - loss: 0.1198 - acc: 0.9680\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 7s 1ms/sample - loss: 0.2023 - acc: 0.9602\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 671us/sample - loss: 0.1510 - acc: 0.9680\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1231 - acc: 0.9655\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 668us/sample - loss: 0.0969 - acc: 0.9728\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1329 - acc: 0.9605\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 659us/sample - loss: 0.1050 - acc: 0.9673\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.2617 - acc: 0.9645\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 684us/sample - loss: 0.2352 - acc: 0.9713\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1414 - acc: 0.9628\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 682us/sample - loss: 0.1122 - acc: 0.9685\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1408 - acc: 0.9587\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 665us/sample - loss: 0.1128 - acc: 0.9637\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1390 - acc: 0.9618\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 671us/sample - loss: 0.1092 - acc: 0.9675\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1533 - acc: 0.9632\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 675us/sample - loss: 0.1204 - acc: 0.9682\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1628 - acc: 0.9607\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 677us/sample - loss: 0.1324 - acc: 0.9647\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1457 - acc: 0.9635\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 675us/sample - loss: 0.1154 - acc: 0.9687\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1478 - acc: 0.9627\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 694us/sample - loss: 0.1166 - acc: 0.9683\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1445 - acc: 0.9633\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 708us/sample - loss: 0.1075 - acc: 0.9692\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1457 - acc: 0.9622\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 4s 677us/sample - loss: 0.1235 - acc: 0.9683\n",
      "Train on 6000 samples\n",
      "Epoch 1/2\n",
      "6000/6000 [==============================] - 6s 1ms/sample - loss: 0.1925 - acc: 0.9663\n",
      "Epoch 2/2\n",
      "6000/6000 [==============================] - 4s 682us/sample - loss: 0.1496 - acc: 0.9705\n",
      "Train on 5678 samples\n",
      "5678/5678 [==============================] - 6s 1ms/sample - loss: 0.2319 - acc: 0.9630\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 6000\n",
    "n = X.shape[0]\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print (f\"Epoch : {epoch}\")\n",
    "    for batch_index in range(batch_size,n,batch_size):\n",
    "        X_ = X[batch_index-batch_size:batch_index]\n",
    "        Y_ = Y.toxic.values[batch_index-batch_size:batch_index]\n",
    "        model.fit(X_,Y_,epochs=2)\n",
    "\n",
    "    X_ = X[batch_index:]\n",
    "    Y_ = Y.toxic.values[batch_index:]\n",
    "    model.fit(X_,Y_,)\n",
    "        \n",
    "notify.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39893it [02:40, 249.05it/s]\n"
     ]
    }
   ],
   "source": [
    "y__pred = [model.predict_classes(i.toarray()) for i in tqdm(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.8414752507279197\n",
      "Recall : 0.6895546129374337\n",
      "Accuracy : 0.9583636226906976\n"
     ]
    }
   ],
   "source": [
    "score(y.toxic.values.reshape(-1,1),y__pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
